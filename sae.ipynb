{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SAEs\n",
    "Sam Tetef and Gareth Tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import transformer_lens\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "hf_token = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "banana_bonanza = load_dataset(\"stetef/Banana-Bonanza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.encoder_layer1 = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.encoder_layer2 = nn.Linear(hidden_dim, latent_dim, bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.decoder_layer1 = nn.Linear(latent_dim, hidden_dim, bias=True)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.decoder_layer2 = nn.Linear(hidden_dim, input_dim, bias=True)\n",
    "\n",
    "    def encode(self, data):\n",
    "        hidden_layer = self.relu1(self.encoder_layer1(data))\n",
    "        return self.relu2(self.encoder_layer2(hidden_layer))\n",
    "    \n",
    "    def decode(self, latent_space):\n",
    "        hidden_layer = self.relu3(self.decoder_layer1(latent_space))\n",
    "        return self.decoder_layer2(hidden_layer)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        encoding = self.encode(data)\n",
    "        reconstruction = self.decode(encoding)\n",
    "        return reconstruction, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, data, beta, sparsity_param):\n",
    "    batch_reconstruction, batch_encoding = model.forward(data)\n",
    "    reconstruction_error = (batch_reconstruction - data).pow(2)\n",
    "    l2_error = einops.reduce(reconstruction_error, 'batch_size input_dim -> batch_size', 'sum').mean()\n",
    "\n",
    "    # l1_error = batch_encoding.sum()\n",
    "    kl_lossifier = nn.KLDivLoss(reduction='sum')  # should have batch size\n",
    "    sparsity_loss = kl_lossifier(sparsity_param, batch_encoding)\n",
    "    return l2_error +  beta * sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# model to analyze\u001b[39;00m\n\u001b[32m      2\u001b[39m model_checkpoint = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mtransformer_lens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHookedTransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:1359\u001b[39m, in \u001b[36mHookedTransformer.from_pretrained\u001b[39m\u001b[34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[39m\n\u001b[32m   1355\u001b[39m     center_unembed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[38;5;66;03m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001b[39;00m\n\u001b[32m   1358\u001b[39m \u001b[38;5;66;03m# match the HookedTransformer parameter names.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m state_dict = \u001b[43mloading\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_pretrained_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[32m   1364\u001b[39m model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m   1365\u001b[39m     cfg,\n\u001b[32m   1366\u001b[39m     tokenizer,\n\u001b[32m   1367\u001b[39m     move_to_device=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1368\u001b[39m     default_padding_side=default_padding_side,\n\u001b[32m   1369\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformer_lens/loading_from_pretrained.py:1856\u001b[39m, in \u001b[36mget_pretrained_state_dict\u001b[39m\u001b[34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001b[39m\n\u001b[32m   1849\u001b[39m         hf_model = T5ForConditionalGeneration.from_pretrained(\n\u001b[32m   1850\u001b[39m             official_model_name,\n\u001b[32m   1851\u001b[39m             torch_dtype=dtype,\n\u001b[32m   1852\u001b[39m             token=huggingface_token \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(huggingface_token) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1853\u001b[39m             **kwargs,\n\u001b[32m   1854\u001b[39m         )\n\u001b[32m   1855\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1856\u001b[39m         hf_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhuggingface_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhuggingface_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# Load model weights, and fold in layer norm weights\u001b[39;00m\n\u001b[32m   1865\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m hf_model.parameters():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:573\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    577\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4455\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4446\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4448\u001b[39m     (\n\u001b[32m   4449\u001b[39m         model,\n\u001b[32m   4450\u001b[39m         missing_keys,\n\u001b[32m   4451\u001b[39m         unexpected_keys,\n\u001b[32m   4452\u001b[39m         mismatched_keys,\n\u001b[32m   4453\u001b[39m         offload_index,\n\u001b[32m   4454\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4455\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4461\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4465\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4475\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4476\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4906\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, low_cpu_mem_usage, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, device_mesh, key_mapping, weights_only, _fast_init)\u001b[39m\n\u001b[32m   4904\u001b[39m         error_msgs += _load_state_dict_into_zero3_model(model_to_load, state_dict, assign_params)\n\u001b[32m   4905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4906\u001b[39m         \u001b[43mmodel_to_load\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[43massign_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4909\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2554\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2555\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2557\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2558\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2559\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2549 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2549\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2543\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2544\u001b[39m         child_state_dict = {\n\u001b[32m   2545\u001b[39m             k: v\n\u001b[32m   2546\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2547\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2548\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2552\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2532\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2531\u001b[39m     local_metadata[\u001b[33m\"\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m\"\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2532\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2542\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/Dropbox/Programs/opensource/stetef/SAE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2438\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   2436\u001b[39m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[32m   2437\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2438\u001b[39m             \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   2440\u001b[39m     action = \u001b[33m\"\u001b[39m\u001b[33mswapping\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcopying\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# model to analyze\n",
    "model_checkpoint = \"meta-llama/Llama-3.2-1B\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run using transformer-lens, check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If 5 workers can complete a job in 12 days, how many days would it take 3 workers to complete the same job?\n",
      " A) 10 B) 12 C) 15 D) 18 E) 20\n",
      "A. 10\n",
      "B. 12\n",
      "C. 15\n",
      "D. 18\n",
      "E. 20\n",
      "Answer: B"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "\n",
    "# take dataset\n",
    "# https://huggingface.co/datasets/stetef/Banana-Bonanza\n",
    "questions = banana_bonanza[\"train\"]['Question']\n",
    "llama32_tokens = model.to_tokens(questions[0])\n",
    "\n",
    "# print(llama32_tokens.shape)\n",
    "# new_token = 29182\n",
    "# print(torch.cat([llama32_tokens, torch.tensor([new_token]).unsqueeze(0).to(\"mps\")], dim=1))\n",
    "print(questions[0])\n",
    "for i in range(150):\n",
    "    # print(llama32_tokens.shape)\n",
    "    # print(llama32_tokens.device)\n",
    "    llama32_logits, _ = model.run_with_cache(llama32_tokens, remove_batch_dim=True)\n",
    "    logits = llama32_logits[0][-1]\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    normalized_distribution = softmax(logits)\n",
    "    next_token = torch.argmax(normalized_distribution).item()\n",
    "    llama32_tokens = torch.cat([llama32_tokens, torch.tensor([next_token]).unsqueeze(0).to(\"mps\")], dim=1)\n",
    "\n",
    "    if next_token == 128001:\n",
    "        break\n",
    "\n",
    "    print(model.to_string(next_token), end='')\n",
    "\n",
    "# run using transformer lens with hooks to train SAE, print loss from time to time\n",
    "# replace transformer activations with trained reconstructed SAE activations\n",
    "# override transformer activations with a particular feature turned on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x TransformerBlock(\n",
      "      (ln1): RMSNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): RMSNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): GroupedQueryAttention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): GatedMLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_pre_linear): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): RMSNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SAE.__init__() missing 3 required positional arguments: 'hidden_dim', 'latent_dim', and 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sae = \u001b[43mSAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: SAE.__init__() missing 3 required positional arguments: 'hidden_dim', 'latent_dim', and 'input_dim'"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "sae = SAE()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
